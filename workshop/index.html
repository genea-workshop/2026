<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>GENEA Workshop 2026
  </title>

  <!-- Bootstrap core CSS -->
  <link href="/2026/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
  <link href="/2026/vendor/fontawesome-free/css/all.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="/2026/css/iva.min.css" rel="stylesheet">
  <style>
    .announcement-box {
      background-color: rgba(125, 188, 255, 0.4);
      color: #777;
      font-size: 14px;
      line-height: 23px;
      padding: 13px 16px;
      text-align: justify;
      font-family: Arial, Helvetica, sans-serif;
      border-radius: 10px;
    }

    .announcement-box .badge {
      background-color: #7A294F;
      color: #fff;
      font-weight: bold;
      text-transform: uppercase;
      text-align: center;
    }
  </style>
</head>

<body id="page-top">

  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
      <span class="d-block d-lg-none">GENEA Challenge 2026</span>
      <!-- <span class="d-none d-lg-block">
        <img src="img/avatar.png" class="img-fluid img-profile rounded-circle mx-auto mb-5" alt="">

      </span> -->
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#home">Home</a>
        </li>

        <!-- <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#workshop-programme"><b>Workshop programme</b></a>
        </li> -->

        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#important-dates">Important dates</a>
        </li>

        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#call-for-papers">Call for papers</a>
        </li>

        <!-- <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#call-for-posters">Call for posters</a>
        </li> -->

        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#reproducibility-award">Reproducibility Award</a>
        </li>

        <!-- <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#invited-speakers">Invited speakers</a>
        </li> -->

        <!-- <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#accepted-papers">Accepted papers</a>
        </li> -->

        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#organising-committee">Organising committee</a>
        </li>
        <!--
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#program-committee">Program committee</a>
        </li>
      -->
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#join-our-community">Join our community</a>
        </li>
      </ul>
    </div>
  </nav>


  <div class="container-fluid p-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="home">
      <div class="w-100">
        <div class="row">
          <div class="col-md-9 col-sm-12">
            <div> <img src="/2026/img/GENEA_logo.png" class="img-fluid rounded" class="mt-2" width=300 alt="GENEA"
                title="GENEA"> </div>
            <div>
              <h2 class="mb-2">
                Workshop 2026
              </h2>
            </div>

            <div class="subheading mb-5">Generation and Evaluation of Non-verbal Behaviour for Embodied Agents</div>

            <!-- <p class="mb-5">
            <div class="col-sm-12 col-md-12">
              <div class="row">
              <div class="announcement-box">
                <span class="badge">Announcement</span>
                ðŸ“¢ <strong>Call for Posters</strong> now open! Submit your work by <strong>September 30, 2026</strong>.
                    <a href="#call-for-posters">Click here for more details</a>.
              </div>
              </div>
            </div>
            </p> -->

            <p class="mb-5">
              <b>Official <a href="https://icmi.acm.org/2026/" target="_blank">ICMI 2026</a> Workshop â€“ October
               2026 (in person)
              </b><br><br>

              The GENEA (Generation and Evaluation of Non-verbal Behaviour for Embodied Agents) Workshop 2026 aims to
              bring together
              researchers from diverse disciplines working on different aspects of non-verbal behaviour generation,
              facilitating
              discussions on advancing both generation techniques and evaluation methodologies.
              We invite contributions from fields such as human-computer interaction, machine learning, multimedia,
              robotics, computer
              graphics, and social sciences. This is the seventh installment of the GENEA Workshop,
              to learn more about the GENEA Initiative, please go <a
                href="https://genea-workshop.github.io/" target="_blank">here</a>.<br><br>

              The workshop proceedings can be found <a
                href="https://dl.acm.org/doi/proceedings/10.1145/3746268" target="_blank">here</a>.<br><br>

            </p>
            <div class="row justify-content-center">

              <img src="/2026/img/avatar.png" class="img-fluid rounded" class="mt-2" width=300 alt="">
            </div>
          </div>
          <div class="col-md-3 d-none d-md-block">
            <a class="twitter-timeline" href="https://twitter.com/genea_workshop?ref_src=twsrc%5Etfw">Tweets by
              genea_workshop</a>
            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
          </div>
        </div>
      </div>

    </section>


    <!-- <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="workshop-programme">
      <div class="w-100">
        <h2 class="mb-5">Planned Workshop programme</h2>
        <h4>ALL TIMES IN DUBLIN' LOCAL TIMEZONE (UTC+1)</h4>
        <div>The Workshop presentations will take place at ACM Multimedia in Dublin on October 28th.</div>
        
        <div class="row">
          <div class="col">09:00&nbsp;-&nbsp;09:10</div>
          <div class="col-10"><b>Opening statement</b></div>
        </div>

        <div class="row">
          <div class="col">09:10&nbsp;-&nbsp;10:10</div>
          <div class="col-10"><b>Keynote presentation by Catherine Pelachaud (Sorbonne University, France)</b></div>
        </div>

        <div class="row">
          <div class="col">10:10&nbsp;-&nbsp;10:30</div>
          <div class="col-10"><b>Workshop paper presentations part I</b></div>
        </div>
        <div class="row">
          <ul style="list-style: none">
            <li><span style="margin-right: 10px;"> </span> Evaluating Automatic Hand-Gesture Generation Using Multimodal Corpus Annotations: The Benefits of a Multidisciplinary Approach (MickaÃ«lla Grondin Verdon et al.)</li>
            <li><span style="margin-right: 10px;"> </span> SARGes: Semantically Aligned Reliable Gesture Generation via Intent Chain (Nan Gao et al.)</li>
          </ul>
        </div>

        <div class="row">
          <div class="col">10:30&nbsp;-&nbsp;11:00</div>
          <div class="col-10"><b>Coffee break</b></div>
        </div>

        <div class="row">
          <div class="col">11:00&nbsp;-&nbsp;11:20</div>
          <div class="col-10"><b>GENEA Leaderboard presentation</b></div>
        </div>

        <div class="row">
          <div class="col">11:20&nbsp;-&nbsp;11:40</div>
          <div class="col-10"><b>GENEA Leaderboard discussion</b></div>
        </div>

        <div class="row">
          <div class="col">11:40&nbsp;-&nbsp;12:10</div>
          <div class="col-10"><b>Poster session</b></div>
        </div>
        
        <div class="row">
          <div class="col">12:20&nbsp;-&nbsp;13:20</div>
          <div class="col-10"><b>Lunch break</b></div>
        </div>

        <div class="row">
          <div class="col">13:30&nbsp;-&nbsp;14:30</div>
          <div class="col-10"><b>Keynote presentation by Asli Ozyurek (Max Planck Institute for Psycholinguistics, the Netherlands)</b></div>
        </div>

        <div class="row">
          <div class="col">14:30&nbsp;-&nbsp;15:00</div>
          <div class="col-10"><b>Workshop paper presentations part II</b></div>
        </div>
        <div class="row">
          <ul style="list-style: none">
            <li><span style="margin-right: 10px;"> </span> From Embeddings to Language Models: A Comparative Analysis of Feature Extractors for Text-Only and Multimodal Gesture Generation (Johsac Isbac Gomez Sanchez et al.)</li>
            <li><span style="margin-right: 10px;"> </span> SemGest: A Multimodal Feature Space Alignment and Fusion Framework for Semantic-aware Co-speech Gesture Generation (Yo-Hsin Fang et al.)</li>
          </ul>
        </div>

        <div class="row">
          <div class="col">15:00&nbsp;-&nbsp;15:30</div>
          <div class="col-10"><b>Break / Poster session</b></div>
        </div>



        <div class="row">
          <div class="col"> 15:30&nbsp;-&nbsp;16:30</a> </div>
          <div class="col-10"><b>Panel discussion</b></div>
        </div>
        <div class="row">
          <ul style="list-style: none">
            <li><span style="margin-right: 10px;"> </span>Catherine Pelachaud, Asli Ozyurek, Rachel McDonnell, and Shalini De Mello.</li>
          </ul>
        </div>
      

        <div class="row">
          <div class="col">16:30&nbsp;-&nbsp;17:00</div>
          <div class="col-10"><b>Closing remarks</b></div> (announce award winner)
        </div>

      </div>

    </section> -->

    <!-- <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="important-dates">
      <div class="w-100">
        <h2 class="mb-5">Important dates </h2>
        <p>Submission Deadlines: All deadlines are set at the end of the day, <a
            href="https://en.wikipedia.org/wiki/Anywhere_on_Earth">Anywhere on Earth (AoE)</a></p>
        <div class="row">
          <div class="col">July 9, 2026</div>
          <div class="col">Paper abstract deadline</div>
        </div>
        <div class="row">
          <div class="col">July 11, 2026</div>
          <div class="col">Submission deadline</div>
        </div>
        <div class="row">
          <div class="col">August 01, 2026</div>
          <div class="col">Notification of paper acceptance</div>
        </div>
        <div class="row">
          <div class="col">August 15, 2026</div>
          <div class="col">Camera-ready deadline</div>
        </div>
        <div class="row">
          <div class="col">Sept 19, 2026</div>
          <div class="col">Poster-session submission deadline</div>
        </div>
        <div class="row">
          <div class="col">Oct 3, 2023</div>
          <div class="col">Notification of poster acceptance</div>
        </div>
        <div class="row">
          <div class="col">Oct 28, 2026</div>
          <div class="col">In-person workshop at ACM Multimedia</div>
        </div>
    </section> -->


    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="call-for-papers">

      <div class="w-100">
        <h2 class="mb-5">Call for papers</h2>

        <div class="iva-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="iva-content">
            <h4>Comming Soon!</h4>
            <p>
              We are currently preparing our Call for Papers and will update this section shortly with all the details. 
              We look forward to receiving your paper submissions. Stay tuned for updates!
            </p>
          </div>

          <!-- <div class="iva-content">
            <p>GENEA 2026 is the seventh GENEA Workshop and an official workshop of ICMI â€™26, which will take place in
              Napoli,
              Italy. Accepted paper submissions will be included in dedicated proceedings by the ACM.</p>

            <p>Generating non-verbal behaviours, such as gesticulation, facial expressions and gaze, is of great
              importance for
              natural interaction with embodied agents such as virtual agents and social robots. At present, behaviour
              generation is
              typically powered by rule-based systems, data-driven approaches like generative AI, or their hybrids. For
              evaluation,
              both objective and subjective methods exist, but their application and validity are frequently a point of
              contention.
            </p>

            <p>The aim of the GENEA Workshop is to bring together researchers working on the generation and evaluation
              of non-verbal
              behaviours for embodied agents. The goal is to</p>


            <ul>
              <li>facilitate knowledge transfer and discussion across different communities and research fields;</li>
              <li>promote data, resources, evaluation, and reproducibility, also evolving best practices in these areas;
                and</li>
              <li>provide an inclusive environment where new and established researchers can learn from each other.</li>
            </ul>

            <p>To kickstart this process, we invite all interested researchers to submit a paper or a poster for
              presentation at the
              workshop, and to attend the event.</p>

            <h4>Paper topics include (but are not limited to) the following</h4>
            <ul>
              <li>Automated synthesis of facial expressions, gestures, and gaze movements, including multi-modal
                synthesis</li>
              <li>Audio-, music- and emotion-driven or stylistic non-verbal behaviour synthesis</li>
              <li>Closed-loop/end-to-end non-verbal behaviour generation (from perception to action)</li>
              <li>Non-verbal behaviour synthesis in two-party and group interactions</li>
              <li>Using LLMs/VLMs in the context of non-verbal behaviour synthesis</li>
              <li>New datasets, annotation methods, and analyses of existing datasets related to non-verbal behaviour
              </li>
              <li>Cross-cultural and multilingual influences on non-verbal behaviour generation</li>
              <li>Cognitive and affective models for non-verbal behaviour generation</li>
              <li>Social perception and attribution of synthesised non-verbal behaviour</li>
              <li>Ethical considerations and biases in non-verbal behaviour synthesis</li>
              <li>Subjective and objective evaluation methods for all of the above topics</li>
            </ul>

            <h4>Submission guidelines</h4>
            <p>We will accept <b>long</b> (8 pages) and <b>short</b> (4 pages) paper submissions, all in the same
              double-column <a href="https://acmmm2026.org/call-for-papers/" target="_blank">ACM conference
                format</a> as used by ACM MM . Pages containing only references do not
              count toward the page limit for any of the paper types. Submissions should be made in PDF format through
              OpenReview
              and formatted for double-blind review.</p>

            <p><b>Submission site:</b>
              <a href="https://openreview.net/group?id=acmmm.org/ACMMM/2026/Workshop/GENEA">
                https://openreview.net/group?id=acmmm.org/ACMMM/2026/Workshop/GENEA</a>
            </p>

            <p>To encourage authors to make their work reproducible and reward the effort that this requires, we have
              introduced the <a class="js-scroll-trigger" href="#reproducibility-award">GENEA Workshop Reproducibility
                Award</a>.
            </p>

            <p>We will also host an <b>open poster session</b> for advertising your late-breaking results and
              already-published work to the community.
              No paper submission is needed to participate in the poster session, and these posters will not be part of
              any proceedings (non archival).
              Submission guidelines for the poster session will be available on the workshop website.
            </p>
          </div> -->

        </div>
      </div>
    </section>


    <!-- <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="call-for-posters">

      <div class="w-100">
        <h2 class="mb-5">Call for posters</h2>

        <div class="iva-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="iva-content">
            <p>The GENEA Workshop at ACM Multimedia 2026 will host an open poster session for advertising your
              late-breaking
              results and recently-published work to the community. Only a poster submission is required, no paper
              submission is needed to participate in the poster session, and these posters will not be part of any
              proceedings (i.e., non-archival). However, poster presentation does require a valid registration with ACM
              Multimedia to
              attend the workshop, and is subject to space constraints.</p>

            <h4>Paper topics include (but are not limited to) the following</h4>
            <ul>
              <li>Automated synthesis of facial expressions, gestures, and gaze movements, including multi-modal
                synthesis</li>
              <li>Audio-, music- and emotion-driven or stylistic non-verbal behaviour synthesis</li>
              <li>Closed-loop/end-to-end non-verbal behaviour generation (from perception to action)</li>
              <li>Non-verbal behaviour synthesis in two-party and group interactions</li>
              <li>Using LLMs/VLMs in the context of non-verbal behaviour synthesis</li>
              <li>New datasets, annotation methods, and analyses of existing datasets related to non-verbal behaviour
              </li>
              <li>Cross-cultural and multilingual influences on non-verbal behaviour generation</li>
              <li>Cognitive and affective models for non-verbal behaviour generation</li>
              <li>Social perception and attribution of synthesised non-verbal behaviour</li>
              <li>Ethical considerations and biases in non-verbal behaviour synthesis</li>
              <li>Subjective and objective evaluation methods for all of the above topics</li>
            </ul>

            <h3>Poster guidelines</h3>
            <ul>
              <li>Poster format: 1-page poster (no larger than A0 size; portrait is recommended). There is no specific
                template. The poster can be designed as you want.
              </li>
              <li>How to submit: please submit your poster draft (in PDF format) and/or poster abstract <a
                  href="https://forms.gle/TqeTZyP7TSpsXJTXA">here (https://forms.gle/TqeTZyP7TSpsXJTXA)</a>.
                We will acknowledge your submission within 24 hours. The submission deadline is 23:59, September 19,
                2026 (Anywhere
                on Earth timezone). We will get back to you no later than October 1st to let you know if we are able to
                accommodate your poster at the event.</li>
            </ul>
          </div>
        </div>
      </div>
    </section> -->

    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="reproducibility-award">

      <div class="w-100">
        <h2 class="mb-5">Reproducibility Award</h2>
        Reproducibility is a cornerstone of the scientific method. Lack of reproducibility is a serious issue in
        contemporary research which we want to address at our workshop. To encourage authors to make their papers
        reproducible, and to reward the effort that reproducibility requires, we are introducing the GENEA Workshop
        Reproducibility Award. All short and long papers presented at the GENEA Workshop will be eligible for this
        award. Please note that it is the camera-ready version of the paper which will be evaluated for the reward.
        <br><br>
        The award is awarded to the paper with the greatest degree of reproducibility. The assessment criteria include:
        <ul>
          <li>ease of reproduction (ideal: just works, if there is code - it is well documented and we can run it)</li>
          <li>extent (ideal: all results can be verified)</li>
          <li>data accessibility (ideal: all data used is publicly available)</li>
        </ul>

        <!-- This year saw exceptional efforts in reproducibility across submissions, raising the overall standard of transparency and reliability in research.  
        <br>
        <br>
        <div style="text-align: center; margin-top: 30px;">
          This year's award is awarded to: <a href="https://openreview.net/forum?id=DhgHmACPpI" target="_blank"><b>From Embeddings to Language Models: A Comparative Analysis of Feature Extractors for Text-Only and Multimodal Gesture Generation</b></a> <br> by
          <i>Johsac Isbac Gomez Sanchez, Paula Dornhofer Paro Costa</i>. <br><br>
          <img src="/2026/img/award.JPG" alt="reproducibility award" width=600>
        </div>
        <br><br>
      </div> -->

    </section>

    <!-- <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="invited-speakers">
      <div class="w-100">
        <h2 class="mb-5">Invited speakers</h2>
        <h3 class="mt-5" id="catherine"><a href="http://chronos.isir.upmc.fr/~pelachaud/" target="_blank">Catherine Pelachaud</a></h3>
        <div class="row">
          <div class="col-2"><a href="http://chronos.isir.upmc.fr/~pelachaud/" target="_blank"><img
                src="/2026/img/2024_catherine_pelachaud.jpg" style=" margin-right: 30px; margin-bottom: 10px"
                class="img-fluid rounded" alt="Catherine Pelachaud"></a>
          </div>
          <div class="col-10">
            <h5>Biography</h5>
            Catherine Pelachaud is a CNRS Director of Research in the laboratory ISIR, Sorbonne University. Her research
            interest includes socially interactive agent, nonverbal communication (face, gaze, gesture, and touch), and adaptive
            mechanisms in interaction. 
            With her research team, she has been developing an interactive virtual agent platform,
            Greta, that can
            display emotional and communicative behaviors. She is co-editor of the ACM handbook on socially interactive
            agents
            (2021-22). She is the recipient of the ACM â€“ SIGAI Autonomous Agents Research Award 2015, ICMI Sustained
            Accomplishment
            Award 2024, and was honored with the title Doctor Honoris Causa of the University of Geneva in 2016. Her
            Siggraphâ€™94
            paper received the Influential Paper Award of IFAAMAS (the International Foundation for Autonomous Agents
            and Multiagent
            Systems).

            <br>
            <br>
            <h5>Talk: Adapting Verbal and Nonverbal Behaviors of an SIA</h5>
            During an interaction, participants exhibit a variety of adaptation mechanisms, which can take many forms,
            from signal imitation to synchronization and conversational strategies. The adaptation of the multimodal
            behaviours of a socially interactive agent to the behaviours of its human interlocutors has been
            demonstrated to promote engagement, build rapport and trust, and support the learning process. In this
            presentation, I will introduce computational approaches that have been developed to generate communicative
            and socio-emotional behaviours to convey intentions and affect. During the presentation, I will outline the
            experimental methods we have applied that have been conducted in order to validate these approaches. The
            objective and subjective measures will be introduced. The presentation will focus on Greta, an open-source
            system that facilitates the modelling of human-agent interactions.
          </div>
        </div>
        

        <h3 class="mt-5" id="asli"><a href="https://www.mpi.nl/people/ozyurek-asli" target="_blank">Asli Ozyurek</a></h3>
        <div class="row">
          <div class="col-2"><a href="https://www.mpi.nl/people/ozyurek-asli" target="_blank"> <img
                src="/2026/img/Asli_Ozyurek.jfif" style=" margin-right: 30px; margin-bottom: 10px"
                class="img-fluid rounded" alt="Asli Ozyurek"></a></div>
          <div class="col-10">
            <h5>Biography</h5>
            Prof. Dr. Asli Ã–zyÃ¼rek, since 2022, is the Scientific Director of Multimodal Language Department (with over
            40 scientific members) at the Max Planck Institute for Psycholinguistics of the Max Planck Society. She is
            also a PI at Donders Institute for Brain Cognition and Behavior at Radboud University. Ã–zyÃ¼rek received a
            joint PhD in Linguistics and Psychology from the University of Chicago. She investigates the inherently and
            universal multimodal nature of human language capacity as one of its adaptive design features. To do so she
            studies how humans and machines produce and process gestures with spoken language and sign languages; how
            brain supports multimodal language; how typologically different spoken and signed languages pattern their
            structures given their multimodal diversity, and how cognition, learning constraints and communicative
            pressures of interaction shape multimodal language, its acquisition and evolution as an adaptive system. She
            uses a variety of methodologies such as behavioral and kinematic analyses of multimodal linguistic
            structures, eye tracking, machine learning, computational modeling, virtual reality and brain imaging to
            understand the complex multimodal nature of human language capacity and how it is recruited in different
            communicative settings. She has received many prestigious grants from NSF, NIH, Dutch Science Foundation
            (VIDI;VICI), ERC, EU Commision and Turkish Science Foundation and is an elected fellow of the Cognitive
            Society and Academia Europea.

            <br>
            <br>
            <h5>Talk: What insights can human cognition offer to enhance computational models of multimodal language generation and evaluation?</h5>
            Most models used to generate semantically and contextually appropriate cospeech gesture generation use
            existing speech
            and gesture data sets as ground truth to train and generate novel gestures. These use mostly fusion models
            of motion
            kinematics of gestures with semantics of cooccurring speech. They are recently being elevated by labeling of
            the
            training gesture data by LLMs. While these methods achieve to generate somewhat semantically appropriate
            gestures this
            still remains a challenging task. Especially because in these models what is in gesture is mostly defined by
            what is in
            the contextual speech surrounding gesture and what can be labeled by categories of speech/text. In my talk I
            will offer
            types of knowledge gesture generation models should take into account from what we have learned from how
            humans produce
            and comprehend multimodal language and in interaction to inspire generative models. <br><br>

            Recent advances in psycholinguistics have shown that gestures are partially driven by factors outside of the
            speech
            content they cooccur or can be described with such as 1) semantic categories of objects, actions and events
            they
            represent as well as how humans visually process them 2) the communicative intent they are produced with (
            i.e.,
            regarding their size and velocity) 3) the interlocutorâ€™s speech and gestures (i.e, through alignment,
            feedback or other
            initiated repair) as well as their knowledge state (i.e., common ground). These point to the necessity to
            include
            information about these factors into the training data sets (e.g., by using interactive data sets and
            including labels
            about these interactive factors as well as data from the visual input to gestures ) to enhance the richness
            of the
            information models should take into account.
            Finally I will discuss how evaluation methods of new generative models can be enhanced by what we recently
            found out
            about human gesture understanding and (neuro) processing in context such as sensitivity to speech-gesture
            synchrony ,
            ambiguity resolution, processing of iconicity, prediction and turn taking in multimodal contexts. <br><br>

            I will argue that incorporation of knowledge from human cognition and multimodal language processing into
            generative
            models will make not only virtual models more human like but also more interactive.
          </div>
        </div>
      </div>
    </section> -->



    <!-- <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="accepted-papers">
      <div class="w-100">
        <h2 class="mb-5">Accepted papers</h2>

        <p>
        <h4><a href="https://openreview.net/forum?id=smat0Dhl8q" target="_blank">SARGes: Semantically Aligned Reliable Gesture Generation via Intent Chain</a></h4>
        <i> Nan Gao, Yihua Bao, dongdong weng, Jiayi Zhao, Jia Li, Yan Zhou, Pengfei Wan </i><br><br>
        </p>
        <hr>

        <p>
        <h4><a href="https://openreview.net/forum?id=LZi4PcurMe" target="_blank">SemGest: A Multimodal Feature Space Alignment and Fusion Framework for Semantic-aware Co-speech Gesture Generation</a></h4>
        <i>Yo-Hsin Fang, Vijay John, Yasutomo Kawanishi</i><br><br>
        </p>
        <hr>

        <p>
        <h4><a href="https://openreview.net/forum?id=DhgHmACPpI" target="_blank">From Embeddings to Language Models: A Comparative Analysis of Feature Extractors for Text-Only and Multimodal Gesture Generation</a></h4>
        <i>Johsac Isbac Gomez Sanchez, Paula Dornhofer Paro Costa</i><br><br>
        </p>
        <hr>

        <p>
        <h4><a href="https://openreview.net/forum?id=JVhGZITLAz" target="_blank">Evaluating Automatic Hand-Gesture Generation Using Multimodal Corpus Annotations: The Benefits of a Multidisciplinary Approach</a></h4>
        <i>MickaÃ«lla Grondin Verdon, Domitille Caillat, Louis ABEL, Slim Ouni</i><br><br>
        </p>
        <hr>

      </div>
    </section> -->





    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="organising-committee">
      <div class="w-100">
        <h2 class="mb-5">Organising committee</h2>
        <p>
          The main contact address of the workshop is: <a
            href="mailto:genea-contact@googlegroups.com">genea-contact@googlegroups.com</a>. <br> <br>
        </p>
        <h4>Workshop organisers</h4>

        <div class="row">

          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2026/img/taras.jpg" class="img-fluid rounded" alt="Taras Kucherenko">
              </div>
              <div class="col-7">
                <a href="https://svito-zar.github.io/" target="_blank" style="font-weight: bold;">Taras Kucherenko</a>
                <br>
                Electronic Arts (EA) <br> Sweden
              </div>
            </div>
            <hr>
          </div>

          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2026/img/youngwoo.jpg" class="img-fluid rounded" alt="Youngwoo Yoon">
              </div>
              <div class="col-7">

                <a href="https://youngwoo-yoon.github.io/" target="_blank" style="font-weight: bold;">Youngwoo Yoon</a>
                <br>ETRI <br> South Korea
              </div>
            </div>
            <hr>
          </div>

        </div>


        <div class="row">

          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2026/img/rajmund.png" class="img-fluid rounded" alt="Rajmund Nagy">
              </div>
              <div class="col-7">

                <a href="https://nagyrajmund.github.io/" target="_blank" style="font-weight: bold;">Rajmund Nagy</a>
                <br>
                KTH Royal Institute of Technology <br> Sweden


              </div>
            </div>
            <hr>
          </div>

          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2026/img/gustav.jpeg" class="img-fluid rounded" alt="Gustav Eje Henter">
              </div>
              <div class="col-7">

                <a href="https://people.kth.se/~ghe/" target="_blank" style="font-weight: bold;">Gustav Eje Henter</a>
                <br>
                KTH Royal Institute of Technology <br>
                Motorica AB <br>
                Sweden


              </div>
            </div>
            <hr>
          </div>

        </div>

        <div class="row">

          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2026/img/alice.jpg" width="100%" class="img-fluid rounded" alt="Alice Delbosc">
              </div>
              <div class="col-7">
                <a href="https://pageperso.lis-lab.fr/alice.delbosc/" target="_blank" style="font-weight: bold;">Alice
                  Delbosc</a>
                <br>
                Davi, The Humanizers <br> France
              </div>
            </div>
            <hr>
          </div>

          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2026/img/laura.jfif" class="img-fluid rounded" alt="Laura Hensel">
              </div>
              <div class="col-7">

                <a href="https://scholar.google.co.uk/citations?user=1R8GM7sAAAAJ&hl=en" target="_blank"
                  style="font-weight: bold;">Laura Hensel</a>
                <br>
                University of Glasgow <br> Scotland, UK
              </div>
            </div>
            <hr>
          </div>

        </div>

        <div class="row">

          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2026/img/oyaceliktutan.jfif" width="100%" class="img-fluid rounded" alt="Oya Celiktutan">
              </div>
              <div class="col-7">
                <a href="https://nms.kcl.ac.uk/oya.celiktutan/" target="_blank" style="font-weight: bold;">Oya
                  Celiktutan</a>
                <br>
                King's College London <br> United Kingdom
              </div>
            </div>
            <hr>
          </div>

        </div>

      </div>
    </section>

    <!-- <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="program-committee">
      <div class="w-100">
        <h2 class="mb-5">Program committee</h2>
        <ul>
                  <li>None yet.</li>
                </ul>
      </div>

    </section> -->
    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="join-our-community">
      <div class="w-100">
        <h2 class="mb-5">Join our community</h2>
        <p>Join our <b>Slack</b> space dedicated to the gesture generation research community.
          In order to ensure the security and integrity of our community, we kindly ask you to fill out
          <a href="https://docs.google.com/forms/d/e/1FAIpQLSfFq0-lYGVwDVNmxlXEESk2_cYsvsRjzndR8Mu6QCGuFXavVA/viewform"
            style="font-weight: bold;" target="_blank">this form</a>
          to obtain the invitation link.<br>
          Join us to share your work, data, interesting papers, questions, ideas, job opportunities and more.
        </p>

        <p>And keep up to date with the latest workshop news by following us on <a href="https://x.com/genea_workshop"
            style="font-weight: bold;" target="_blank">X</a>.</p>
      </div>


  </div>

  <!-- Bootstrap core JavaScript -->
  <script src="/2026/vendor/jquery/jquery.min.js"></script>
  <script src="/2026/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="/2026/vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="/2026/js/iva.min.js"></script>

</body>

<!-- Panelbear -->
<script async src="https://cdn.panelbear.com/analytics.js?site=9bGH0f0hxBy"></script>
<script>
  window.panelbear = window.panelbear || function () { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
  panelbear('config', { site: '9bGH0f0hxBy' });
</script>

</html>
